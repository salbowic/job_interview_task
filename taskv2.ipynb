{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a an LLM pipeline that will transform any free text query into a SQL query, the key points of this task are:\n",
    "\n",
    "* Create a valid representation of SQL tables allowing for semantic search that will match the top results to the given free text query\n",
    "\n",
    "* Based on the table representation the LLM has to create a real SQL query, based on free text user query, that will allow for immediate usage\n",
    "\n",
    "* LLM has to support creation of queries with different levels of complexity not only the simplest ones\n",
    "\n",
    "* LLM has to support creating queries to fetch data from different database schemas\n",
    "\n",
    "* When an error in LLM created sql query is encountered it should attempt to self correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 485 tensors from sqlcoder2-GGUF\\sqlcoder2.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = starcoder\n",
      "llama_model_loader: - kv   1:                               general.name str              = StarCoder\n",
      "llama_model_loader: - kv   2:                   starcoder.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                 starcoder.embedding_length u32              = 6144\n",
      "llama_model_loader: - kv   4:              starcoder.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   5:                      starcoder.block_count u32              = 40\n",
      "llama_model_loader: - kv   6:             starcoder.attention.head_count u32              = 48\n",
      "llama_model_loader: - kv   7:          starcoder.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:     starcoder.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<fim_prefix>\", \"<f...\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.scores arr[f32,49152]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,49152]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  322 tensors\n",
      "llama_model_loader: - type q8_0:  163 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 19/49152 vs 0/49152 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = starcoder\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48891\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 6144\n",
      "llm_load_print_meta: n_head           = 48\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 48\n",
      "llm_load_print_meta: n_embd_k_gqa     = 128\n",
      "llm_load_print_meta: n_embd_v_gqa     = 128\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 15B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 15.82 B\n",
      "llm_load_print_meta: model size       = 15.66 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = StarCoder\n",
      "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 145 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.46 MiB\n",
      "llm_load_tensors: offloading 30 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 30/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 16037.15 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 11530.58 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    10.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    30.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   40.00 MiB, K (f16):   20.00 MiB, V (f16):   20.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   426.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    28.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1489\n",
      "llama_new_context_with_model: graph splits = 145\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'StarCoder', 'general.architecture': 'starcoder', 'starcoder.context_length': '8192', 'starcoder.attention.head_count_kv': '1', 'starcoder.embedding_length': '6144', 'starcoder.feed_forward_length': '24576', 'starcoder.block_count': '40', 'starcoder.attention.head_count': '48', 'starcoder.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '0', 'general.file_type': '7', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(model_path=\"sqlcoder2-GGUF\\sqlcoder2.Q8_0.gguf\",\n",
    "               n_batch=512,\n",
    "               n_ctx=2048,\n",
    "               n_gpu_layers=30,\n",
    "               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_statements = \"\"\"\n",
    "-- Create schemas\n",
    "CREATE SCHEMA public;\n",
    "CREATE SCHEMA sales;\n",
    "CREATE SCHEMA analytics;\n",
    "\n",
    "-- Create tables in the public schema\n",
    "CREATE TABLE public.employees (\n",
    "    employee_id INTEGER PRIMARY KEY,\n",
    "    first_name VARCHAR,\n",
    "    last_name VARCHAR,\n",
    "    age INTEGER,\n",
    "    department_id INTEGER,\n",
    "    hire_date DATE,\n",
    "    FOREIGN KEY (department_id) REFERENCES public.departments(department_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE public.departments (\n",
    "    department_id INTEGER PRIMARY KEY,\n",
    "    department_name VARCHAR,\n",
    "    manager_id INTEGER,\n",
    "    FOREIGN KEY (manager_id) REFERENCES public.employees(employee_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE public.salaries (\n",
    "    employee_id INTEGER PRIMARY KEY,\n",
    "    salary_amount DECIMAL,\n",
    "    effective_date DATE,\n",
    "    FOREIGN KEY (employee_id) REFERENCES public.employees(employee_id)\n",
    ");\n",
    "\n",
    "-- Create tables in the sales schema\n",
    "CREATE TABLE sales.orders (\n",
    "    order_id INTEGER PRIMARY KEY,\n",
    "    order_date DATE,\n",
    "    customer_id INTEGER,\n",
    "    sales_rep_id INTEGER,\n",
    "    FOREIGN KEY (customer_id) REFERENCES sales.customers(customer_id),\n",
    "    FOREIGN KEY (sales_rep_id) REFERENCES sales.sales_reps(sales_rep_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE sales.customers (\n",
    "    customer_id INTEGER PRIMARY KEY,\n",
    "    customer_name VARCHAR,\n",
    "    contact_number VARCHAR\n",
    ");\n",
    "\n",
    "CREATE TABLE sales.sales_reps (\n",
    "    sales_rep_id INTEGER PRIMARY KEY,\n",
    "    first_name VARCHAR,\n",
    "    last_name VARCHAR,\n",
    "    region VARCHAR\n",
    ");\n",
    "\n",
    "CREATE TABLE sales.products (\n",
    "    product_id INTEGER PRIMARY KEY,\n",
    "    product_name VARCHAR,\n",
    "    price DECIMAL\n",
    ");\n",
    "\n",
    "-- Create tables in the analytics schema\n",
    "CREATE TABLE analytics.sales_reports (\n",
    "    report_id INTEGER PRIMARY KEY,\n",
    "    report_date DATE,\n",
    "    total_sales DECIMAL,\n",
    "    region VARCHAR\n",
    ");\n",
    "\n",
    "CREATE TABLE analytics.customer_metrics (\n",
    "    metric_id INTEGER PRIMARY KEY,\n",
    "    customer_id INTEGER,\n",
    "    lifetime_value DECIMAL,\n",
    "    average_order_value DECIMAL,\n",
    "    FOREIGN KEY (customer_id) REFERENCES sales.customers(customer_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE analytics.product_performance (\n",
    "    performance_id INTEGER PRIMARY KEY,\n",
    "    product_id INTEGER,\n",
    "    sales_quantity INTEGER,\n",
    "    revenue_generated DECIMAL,\n",
    "    FOREIGN KEY (product_id) REFERENCES sales.products(product_id)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Generate a SQL query to answer this question: `{user_question}`\n",
    "{instructions}\n",
    "\n",
    "DDL statements:\n",
    "{ddl_statements}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "The following SQL query best answers the question `{user_question}`:\n",
    "```sql\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Verify if this SQL query correctly answers the question: {user_question}.\n",
    "SQL query: {sql_query}\n",
    "If yes, return the same query. If not return corrected query.\n",
    "\n",
    "DDL statements:\n",
    "{ddl_statements}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "The following SQL query best answers the question `{user_question}`:\n",
    "```sql\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Select the name and age of employees who joined after January 2020\"\n",
    "instructions = \"\"\"\n",
    "If the question does not match the existing tables, return 'The query does not match any existing tables. Please check the table names or columns and try again.'\n",
    "When an error in your sql query is encountered, attempt to correct it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt = PromptTemplate(template=template, input_variables=[\"user_question\", \"instructions\", \"ddl_statements\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_prompt = PromptTemplate(template=verification_template, input_variables=[\"user_question\", \"sql_query\", \"ddl_statements\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_query_from_llm(prompt, llm, user_question, instructions, ddl_statements):\n",
    "    llm_chain = prompt | llm\n",
    "    raw_llm_answer = llm_chain.invoke({\"user_question\": user_question, \"instructions\": instructions, \"ddl_statements\": ddl_statements})\n",
    "    return raw_llm_answer.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify and correct SQL queries\n",
    "def verify_and_correct_sql(sql_query, user_question, ddl_statements):\n",
    "    llm_chain = verification_prompt | llm\n",
    "    raw_llm_answer = llm_chain.invoke({\"user_question\": user_question, \"sql_query\": sql_query, \"ddl_statements\": ddl_statements})\n",
    "    return raw_llm_answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   27671.74 ms\n",
      "llama_print_timings:      sample time =       8.32 ms /    61 runs   (    0.14 ms per token,  7335.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38836.99 ms /   719 tokens (   54.02 ms per token,    18.51 tokens per second)\n",
      "llama_print_timings:        eval time =   19440.35 ms /    60 runs   (  324.01 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:       total time =   58347.89 ms /   779 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT e.first_name, e.last_name, e.age FROM public.employees AS e JOIN sales.orders so ON e.employee_id = so.sales_rep_id WHERE so.order_date > '2020-01-01';\n"
     ]
    }
   ],
   "source": [
    "sql_query = get_sql_query_from_llm(generation_prompt, llm, user_question, instructions, ddl_statements)\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   27671.74 ms\n",
      "llama_print_timings:      sample time =       8.21 ms /    61 runs   (    0.13 ms per token,  7425.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33543.57 ms /   721 tokens (   46.52 ms per token,    21.49 tokens per second)\n",
      "llama_print_timings:        eval time =   21383.00 ms /    60 runs   (  356.38 ms per token,     2.81 tokens per second)\n",
      "llama_print_timings:       total time =   54997.98 ms /   781 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT e.first_name, e.last_name, e.age FROM sales.employees AS e JOIN sales.orders so ON e.employee_id = so.sales_rep_id WHERE so.order_date > '2020-01-01';\n"
     ]
    }
   ],
   "source": [
    "corrected_sql_query = verify_and_correct_sql(sql_query, user_question, ddl_statements)\n",
    "print(corrected_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sql_pipe(user_question, instructions=instructions, ddl_statements=ddl_statements):\n",
    "    # Generate the initial SQL query\n",
    "    sql_query = get_sql_query_from_llm(generation_prompt, llm, user_question, instructions, ddl_statements)\n",
    "    print(f\"Generated SQL Query:\\n{sql_query}\\n\")\n",
    "    \n",
    "    # Verify and potentially correct the SQL query\n",
    "    final_sql_query = verify_and_correct_sql(sql_query, user_question, ddl_statements)\n",
    "    print(f\"Final SQL Query After Verification and Correction:\\n{final_sql_query}\\n\")\n",
    "    \n",
    "    return final_sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sql_query = text_to_sql_pipe(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query(natural_language_query):\n",
    "    try:\n",
    "        sql_query = llm.generate(natural_language_query)\n",
    "        return sql_query\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating SQL query: {e}\")\n",
    "        return None\n",
    "\n",
    "def sql_pipeline(natural_language_query):\n",
    "    sql_query = generate_sql_query(natural_language_query)\n",
    "    return sql_query\n",
    "\n",
    "# Example usage\n",
    "nl_query = \"Select the name and age of employees who joined after January 2020\"\n",
    "sql_query = sql_pipeline(nl_query)\n",
    "print(\"Generated SQL Query:\", sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
